
dataset folder
---------------
<strong>CTR_30.zip</strong><br>
: It contains 30 text files, each of which was extracted from a webpage about Hurricane Isaac disaster.  This is a homogeneous dataset.

<strong>NYT_1000_IDs_EXP3.txt</strong><br>
: It contains 1000 document IDs of the New York Times articles used in this study.  Considering that the NYT corpus is a proprietary, we provide only the IDs, and not the actual text content.

<strong>NYT_1000_taggedTopics_EXP3.sql</strong><br>
: This is a database table dump, which contains various topics extracted from the NYT_1000 dataset using diverse Xpantrac settings with 3 different search APIs, as well as the baseline, TF*IDF, and OpenCalais. 

<strong>RelevanceRatingBy3HumanRaters_EXP2_CTR_and_VARIOUS_30(xpantrac,tfidf topics).xlsx</strong><br>
: This file contains ratings for each topic, which was extracted by using Xpantrac as well as a baseline, TF*IDF, from both CTR_30 and VARIOUS_30 data sets.  The 5-point Likert scale was used for the relevance rating by three human raters.

<strong>TagsBy3HumanIndexers_EXP1_CTR_30.txt</strong><br>
: It contains topic tags, indexed by 3 humans, for each of 30 documents in the CTR_30 dataset.

<strong>TagsBy5HumanIndexers_EXP1_VARIOUS_30.txt</strong><br>
: It contains topic tags, indexed by 5 humans, for each of 30 documents in the VARIOUS_30 dataset.

<strong>VARIOUS_30.zip</strong><br>
: It contains 30 text files about diverse content including natural and man-made disasters, health issues, civil conflicts, and epidemics. This is a heterogeneous dataset.


code folder
---------------
<strong>custom_stops.txt</strong><br>
: It contains additional stopwords.

<strong>pos_tagger.py</strong><br>
: This script contains part-of-speech taggers that are chained as the backoff taggers of each other, in order to improve the tagging accuracy.

<strong>stopwords.txt</strong><br>
: It contains a list of general purpose stopwords.

<strong>Xpantrac_bing_buildCache.py</strong><br>
: This script expands provided input texts using a commercial search engine API (e.g., Bing API), then stores the expanded textual information in a database table for later analysis and topic extraction.

<strong>Xpantrac_bing_DB.py</strong><br>
: This script extracts the expanded textual information from a database table, which had been stored using Xpantrac_bing_buildCache.py.  One of the benefits is to extract topics for each varying parameter setting, for example, different values of the "number of API return" parameter.

<strong>Xpantrac_extractTopics_bing.py</strong><br>
: This script is an autonomous topic extraction script to identify topic tags given textual documents.  The Bing Azure API was used in this script.


script folder
---------------
<strong>compute_IIC_human_machine.py</strong><br>
: It computes the Rolling's Inter-Indexer Consistency (IIC) between topic groups, which were identified by human indexers and by Xpantrac.

<strong>compute_IIC_human_opencalais.py</strong><br>
: It computes the Rolling's Inter-Indexer Consistency (IIC) between topic groups, which were identified by human indexers and by OpenCalais NLP API.

<strong>custom_stops.txt</strong><br>
: It contains additional stopwords.

<strong>stopwords.txt</strong><br>
: It contains a list of general purpose stopwords.

<strong>topic_extraction_opencalais.py</strong><br>
: This script extracts topics using the OpenCalais NLP API.

<strong>topic_extraction_TFIDF_CTR30,VARIOUS_30.py</strong><br>
: It extracts topics by using the baseline, TF*IDF, from both CTR_30 and VARIOUS_30 datasets.

<strong>topic_extraction_TFIDF_NYT_1000.py</strong><br>
: It extracts topic tags by using the baseline, TF*IDF, from the NYT_1000 dataset.

<strong>topic_tag_evaluator.py</strong><br>
: This script computes the metrics such as precision, recall, and F1 of topics generated by Xpantrac with various settings and used search APIs, for the NYT_1000 dataset.


survey folder
---------------
<strong>DemographicsQuestionnaire_EXP1.docx</strong><br>
: a demographic survey form used for the Experiment 1.

<strong>DemographicsQuestionnaire_EXP4.pdf</strong><br>
: a demographic survey form used for the Experiment 4.

<strong>ExitQuestionnaire_EXP4.pdf</strong><br>
: It asks for mostly the usefulness and satisfaction of the Xpantrac User Interface.

<strong>TaskQuestionnaire_EXP1_CTR30.docx</strong><br>
: It is a data collection tool, and three human indexers assigned topic tags for each document in CTR_30.

<strong>TaskQuestionnaire_EXP2_CTR30.pdf</strong><br>
: Users assigned the topic relevance rating using this questionnaire for the CTR_30 dataset.

<strong>TaskQuestionnaire_EXP2_VARIOUS_30.pdf</strong><br>
: Users assigned the topic relevance rating using this questionnaire for the VARIOUS_30 dataset. 

<strong>TaskQuestionnaire_EXP4_TYPE1.docx</strong><br>
: It includes instructions to use the Xpantrac UI, an exercise task, and three main tasks. Different questionnaire types contain different set of the documents selected from the CTR_30, VARIOUS_30, and NYT_1000 datasets.

<strong>TaskQuestionnaire_EXP4_TYPE2.docx</strong><br>
: Same as the description above.

<strong>TaskQuestionnaire_EXP4_TYPE3.docx</strong><br>
: Same as the description above.

<strong>TaskQuestionnaire_EXP4_TYPE4.docx</strong><br>
: Same as the description above.

<strong>TaskQuestionnaire_EXP4_TYPE5.docx</strong><br>
: Same as the description above.

<strong><strong>TaskQuestionnaire_EXP4_TYPE6.docx</strong><br>
: Same as the description above.

<strong>TaskQuestionnaire_EXP4_TYPE7.docx</strong><br>
: Same as the description above.

<strong>XpantracTutorial.pdf</strong><br>
: This is a tutorial for using Xpantrac.  It was developed for conducting the Experiment 4 (usability study).
